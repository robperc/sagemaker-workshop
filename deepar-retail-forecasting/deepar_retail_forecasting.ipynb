{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Retail Sales Forecasting with DeepAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forecasting is a central problem in many businesses. In the retail industry probabilistic forecasts are important for inventory management to ensure that there is enough product on-hand to meet the seasonal spikes in sales for differnet categories of products. \n",
    "\n",
    "Most forecasting methods have been developed in the setting of forecasting individual time series where model parameters are independently estimated from past observations for each given time series. Today retailers are faced with forecasting demand on potentially millions of time series for different products across their catalog. Retailers also face cold start problems where they need to forecast for a new item that has no existing time series data. \n",
    "\n",
    "In this notebook we will see how the [DeepAR forecasting algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html) can help retailers solve these business problems. Using a generated dataset of daily sales for a set of clothing products we will see how DeepAR can learn jointly across the related time series to capture complex group dependent behavior at a categorical level. Finally we will see how this learned categorical behavior can be used to forecast for products with existing time series as well as new \"cold\" products with no existing data.\n",
    "\n",
    "For a more rigorous explaination of the DeepAR algorithm check out the [DeepAR white paper](https://pdfs.semanticscholar.org/4eeb/e0d12aefeedf3ca85256bc8aa3b4292d47d9.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing modules and defining helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modules and helper functions below will be used throughout this lab to generate the dataset and convert data between formats. You do not need to read over and understand each function to proceed with the lab but comments have been added for the inquisitive to reference. Run the below cell before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "import datetime\n",
    "import tempfile\n",
    "import random\n",
    "import boto3\n",
    "import copy\n",
    "import uuid\n",
    "import json\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def make_product(categories, num_years):\n",
    "    # Creates a random product with a unique product id from the list of potential category and subcategory pairings\n",
    "    product_id = str(uuid.uuid4().fields[-1])\n",
    "    cat, subcats = random.choice(list(categories.items()))\n",
    "    subcat = random.choice(subcats)\n",
    "    start_date = datetime.date.today() - datetime.timedelta(random.randint(num_years/2, num_years)*365)\n",
    "    return {'product_id': product_id, \n",
    "            'category': cat, \n",
    "            'subcategory': subcat, \n",
    "            'start_date': start_date}\n",
    "\n",
    "def make_weights(categories, num_years):\n",
    "    # Helper function, creates normalized weights in the interval 0-1 for each input category\n",
    "    end_year = datetime.date.today().year\n",
    "    results = defaultdict(lambda: defaultdict(dict))\n",
    "    for year in range(end_year-num_years, end_year+1):\n",
    "        for month in range(1, 13):\n",
    "            rands = np.random.random(size=len(categories))\n",
    "            weights = rands / rands.sum()\n",
    "            weight_index = 0\n",
    "            for category in categories:\n",
    "                results[category][year][month] = weights[weight_index]\n",
    "                weight_index += 1\n",
    "    return results\n",
    "\n",
    "def make_category_weights(categories, num_years):\n",
    "    # Creates random weights for each category and subcategory to further augment seasonality within these groupings\n",
    "    # Weights are randomly created for every category and subcategory for each year and month\n",
    "    # At the category and subcategory levels the weights are normalized to sum to 1\n",
    "    category_weights = make_weights(list(categories.keys()), num_years) # Create weights for the top level categories\n",
    "    for category, subcategories in categories.items(): # Add weights for the corresponding subcategories\n",
    "        subcat_weights = make_weights(subcategories, num_years)\n",
    "        for subcat, weights in subcat_weights.items():\n",
    "            category_weights[category][subcat] = weights\n",
    "    return category_weights\n",
    "\n",
    "def make_num_returns(date, cat, subcat, returns, weights):\n",
    "    # Helper function, creates daily number of returns for a product based on baseline return seasonality and product weights\n",
    "    month = date.month\n",
    "    year = date.year\n",
    "    monthly_returns = returns[month]\n",
    "    noise = np.random.normal(scale=0.1)\n",
    "    cat_weight = weights[cat][year][month]\n",
    "    subcat_weight = weights[cat][subcat][year][month]\n",
    "    num_returns = (monthly_returns + noise*monthly_returns)*cat_weight*subcat_weight\n",
    "    return int(num_returns)\n",
    "\n",
    "def make_data_for_product(product, returns, weights):\n",
    "    # Helper function, creates time series dataframe for input product based on baseline return seasonality and product weights\n",
    "    cat = product['category']\n",
    "    subcat = product['subcategory']\n",
    "    today = datetime.date.today()\n",
    "    start_date = product['start_date']\n",
    "    delta = today - start_date\n",
    "    data = []\n",
    "    for i in range(delta.days + 1):\n",
    "        local_product = product.copy()\n",
    "        local_product.pop('start_date', None)\n",
    "        date = start_date + datetime.timedelta(days=i)\n",
    "        local_product['date'] = date\n",
    "        local_product['returns'] = make_num_returns(date, cat, subcat, returns, weights)\n",
    "        data.append(local_product)\n",
    "    return pd.DataFrame(data)\n",
    "        \n",
    "\n",
    "def make_data_for_products(products, returns, weights):\n",
    "    # Creates time series return dataframe for all input products based on baseline return seasonality and product weights\n",
    "    df = pd.concat([make_data_for_product(product, returns, weights) for product in products])\n",
    "    df = df[['product_id', 'date', 'category', 'subcategory', 'returns']]\n",
    "    df['product_id'] = df['product_id'].apply(str)\n",
    "    df['date'] = df['date'].apply(lambda t: pd.to_datetime(t, format='%Y-%m-%d'))\n",
    "    return df\n",
    "    \n",
    "def plot_dataset_ts(df, condition):\n",
    "    # Plot summed return values of data at monthly granularity grouped by condition\n",
    "    data = df.copy()\n",
    "    data = data.groupby(['date', condition]).sum().unstack().fillna(0.0)\n",
    "    data = data.groupby([(data.index.year),(data.index.month)]).sum()\n",
    "    fig, ax = plt.subplots(figsize=(15,15))\n",
    "    data.plot(ax=ax)\n",
    "\n",
    "def ts_to_json_obj(series, cat):\n",
    "    ts = series.copy() # Make a local copy of series so as not to modify original dataframe\n",
    "    ts = ts.rename('returns') # Rename series\n",
    "    ts = ts.reset_index() # Input series is indexed by date, re-index to make df with date as column\n",
    "    start_date_index = ts['returns'].nonzero()[0][0] # Pull out index of first non-zero day of return values\n",
    "    ts = ts.iloc[start_date_index:].reset_index(drop=True) # Truncate leading 0 return rows from time series and reset index to 0\n",
    "    json_obj = {\"start\": str(ts['date'][0]), \"cat\": int(cat), \"target\": ts['returns'].astype(int).tolist()}\n",
    "    return json_obj\n",
    "\n",
    "def transform_to_json_objs(df, le):\n",
    "    cats_df = df[['product_id', 'category', 'subcategory']].drop_duplicates().set_index('product_id')\n",
    "    ts_df = df.groupby(['date', 'product_id']).sum().unstack().fillna(0.0)\n",
    "    ts_df.columns = ts_df.columns.droplevel() # Drop unneeded multi-index level\n",
    "    json_objs = []\n",
    "    for column in list(ts_df.columns.values):\n",
    "        cat = cats_df.loc[column, 'category']\n",
    "        subcat = cats_df.loc[column, 'subcategory']\n",
    "        num_cat = le.transform([cat])[0]\n",
    "        num_subcat = le.transform([subcat])[0]\n",
    "        json_objs.append(ts_to_json_obj(ts_df.loc[:, column], num_cat))\n",
    "        json_objs.append(ts_to_json_obj(ts_df.loc[:, column], num_subcat))\n",
    "    return json_objs\n",
    "\n",
    "def make_train_set(json_objs, prediction_length):\n",
    "    objs = copy.deepcopy(json_objs)\n",
    "    for obj in objs:\n",
    "        obj['target'] = obj['target'][:-prediction_length]\n",
    "    return objs\n",
    "\n",
    "def write_to_s3(session, json_objs, prefix, channel):\n",
    "    file_name = '{}.json'.format(channel)\n",
    "    file_path = os.path.join(os.getcwd(), file_name)\n",
    "    with open(file_path, 'wb') as f:\n",
    "        for obj in json_objs:\n",
    "            line = json.dumps(obj) + '\\n'\n",
    "            line = line.encode('utf-8')\n",
    "            f.write(line)\n",
    "        f.seek(0)\n",
    "        f.flush()\n",
    "    return session.upload_data(file_path, key_prefix=prefix)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we will be working with a dataset composed of category + subcategory pairings. The cell below generates a catalog of products using possible category + subcategory pairings from the `categories` dictionary of categories and subcategories. Each product is given a unique product ID to be identified by.\n",
    "\n",
    "Next each product category and subcategory is given a random normalized weight for each month of each year in the time series range. These weights are used to simulate seasonality in different product categories and subcategories, such as boots being more popular in winter than in summer.\n",
    "\n",
    "Lastly, time series sales data is generated for each product using the the product catalog, the normalized weights, and the `seasonality` dictionary, which is used to augment the sales values roughly around what a typical retailers yearly sales trends might look like. To simulate a store adding products over time each product is randomly assigned a \"start\" date in the time series interval in which it begins to have non-zero sales values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categories and subcategories to generate products with\n",
    "# Each product id is generated with a random category and subcategory from that category's options\n",
    "categories = {\n",
    "    'shoe': ['sneaker', 'boot', 'slipper'],\n",
    "    'outerwear': ['coat', 'jacket', 'shell'],\n",
    "    'top': ['shirt', 't-shirt', 'sweater', 'knit'],\n",
    "    'bottom': ['skirt', 'pant', 'short', 'leggings'],\n",
    "    'accessories': ['belt', 'tie', 'scarf', 'hat', 'brooche']\n",
    "}\n",
    "\n",
    "# Seasonality for time series data to be generated around\n",
    "seasonality = {\n",
    "    1: 500,\n",
    "    2: 400,\n",
    "    3: 200,\n",
    "    4: 100,\n",
    "    5: 40,\n",
    "    6: 80,\n",
    "    7: 150,\n",
    "    8: 180,\n",
    "    9: 140,\n",
    "    10: 240,\n",
    "    11: 100,\n",
    "    12: 400\n",
    "}\n",
    "\n",
    "n_products = 100 # Number of products to generate. Increase this to generate more individual product data\n",
    "\n",
    "n_years = 4 # Number of years in past from current date to generate date for. Increase this to generate longer time series for each product\n",
    "\n",
    "products = [make_product(categories, n_years) for i in range(n_products)]\n",
    "\n",
    "category_weights = make_category_weights(categories, n_years)\n",
    "\n",
    "product_data = make_data_for_products(products, seasonality, category_weights)\n",
    "\n",
    "print(product_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have generated our dataset let's take a look at the time series sales values over time at the category and subcategory levels across products. This can be done by running the code in the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize monthly returns of products at category level\n",
    "plot_dataset_ts(product_data, 'category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize monthly returns of products at subcategory level\n",
    "plot_dataset_ts(product_data, 'subcategory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset for DeepAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've generated our raw dataset we need to wrangle it into the format and encoding expected by DeepAR. \n",
    "\n",
    "DeepAR expects data (for training or inference) in [JSON Lines](http://jsonlines.org/) or [Parquet](https://parquet.apache.org/) format. For this lab we'll be working with JSON Lines.\n",
    "\n",
    "In JSON Lines format each line is a seperate JSON object representing a time series for a single product. DeepAR expects each JSON object to have a `start` key, a string or datetime object representing the time the time series data starts at, and a `target` key, whose value is an array of floats (or integers) that represent the time series variable’s values. Additionally each JSON object can include a `cat` key, which is an integer that encodes the categorical grouping that record’s time series is a member of. This allows the model to learn typical behavior for that group and can increase accuracy.\n",
    "\n",
    "Currently our product sales data is in a pandas dataframe and is not yet converted into JSON objects with time series grouped by product ID as the DeepAR algorithm expects for training. In the following cells we'll wrangle the data into the required format.\n",
    "\n",
    "To start, our categorical grouping values (product category and subcategory) are currently strings but DeepAR expects integers for these values. Let's encode our category and subcategory values to integers to use for our `cat` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_cats = product_data.category.unique().tolist()\n",
    "uniq_subcats = product_data.subcategory.unique().tolist()\n",
    "\n",
    "le = LabelEncoder().fit((uniq_cats + uniq_subcats))\n",
    "print(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we've encoded class labels for both the product subcategories and the categories. Later we will see how this allows us to make forecasting predictions for existing product subcategories at a granular level while also allowing us to generalize our predictions to new subcategories that we might wish to introduce by predicting at the higher category level.\n",
    "\n",
    "Next we set the frequency, prediction length, and context length hyperparameters we wish to train our DeepAR model on. \n",
    "\n",
    "Frequency specifies the granularity of the time series in the dataset. In this case out time series values correspond to daily sales results for each product so our frequency is 'D' for daily. Other possible values are 'min' (every minute), 'H' (hourly), 'W' (weekly), and 'M' (monthly).\n",
    "\n",
    "Prediction length controls the number of time steps (based off the unit of frequency) that the model is trained to predict, also called the forecast horizon. Our prediction length is set to '28' to predict roughly a month's worth of days into the future for forecast requests submitted to the trained DeepAR model.\n",
    "\n",
    "Context length controls the the number of time points that the model gets to see before making a prediction. The value for this parameter should be about the same as the prediction_length. The model also receives lagged inputs from the target, so context_length can be much smaller than typical seasonalities. For example, a daily time series can have yearly seasonality. The model automatically includes a lag of one year, so the context length can be shorter than a year. The lag values that the model picks depend on the frequency of the time series. For example, lag values for daily frequency are the previous week, 2 weeks, 3 weeks, 4 weeks, and year.\n",
    "\n",
    "Values for the frequency, prediction length, and context length hyperparameters are required when training a DeepAR model, but you can also configure other optional hyperparameters to further tune your model. For a more exhaustive list of all the different DeepAR hyperparameters you can tune [check out the DeepAR documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freq = 'D'\n",
    "prediction_length = 28\n",
    "context_length = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we transform our pandas dataframe into JSON objects for each product ID as expected by the DeepAR algorithm. This transformation is implemented in the `transform_to_json_objs` helper function created at the start of this notebook for reference.\n",
    "\n",
    "The DeepAR algorithm has an optional test channel for training that can be used to calculate accuracy metrics for the model after training, such as RMSE and quantile loss. For our test set we'll use the full time series for each product. For our training set we'll use the full time series minus our prediction length worth of time points. The loss for our model will then be calculated by how well our model predicts these missing time points in comparison to the ground truth values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_objs = transform_to_json_objs(product_data, le)\n",
    "train_set = make_train_set(json_objs, prediction_length)\n",
    "test_set = json_objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your_username = <YOUR_USERNAME_HERE>\n",
    "your_username = 'robperc'\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "resource_prefix = 'deepar-retail-{}'.format(your_username)\n",
    "\n",
    "train_location = write_to_s3(sagemaker_session, train_set, resource_prefix, 'train')\n",
    "test_location = write_to_s3(sagemaker_session, test_set, resource_prefix, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and deploying a DeepAR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "containers = {\n",
    "    'us-east-1': '522234722520.dkr.ecr.us-east-1.amazonaws.com/forecasting-deepar:latest',\n",
    "    'us-east-2': '566113047672.dkr.ecr.us-east-2.amazonaws.com/forecasting-deepar:latest',\n",
    "    'us-west-2': '156387875391.dkr.ecr.us-west-2.amazonaws.com/forecasting-deepar:latest',\n",
    "    'eu-west-1': '224300973850.dkr.ecr.eu-west-1.amazonaws.com/forecasting-deepar:latest'\n",
    "}\n",
    "\n",
    "image_name = containers[boto3.Session().region_name]\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "s3_output_path = \"{}/{}/output\".format(bucket, resource_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c5.2xlarge',\n",
    "    base_job_name=resource_prefix,\n",
    "    output_path=\"s3://\" + s3_output_path\n",
    ")\n",
    "\n",
    "cardinality = len(le.classes_)\n",
    "\n",
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"num_cells\": \"40\",\n",
    "    \"num_layers\": \"3\",\n",
    "    \"likelihood\": \"student-T\",\n",
    "    \"epochs\": \"50\",\n",
    "    \"mini_batch_size\": \"32\",\n",
    "    \"learning_rate\": \"0.001\",\n",
    "    \"dropout_rate\": \"0.05\",\n",
    "    \"early_stopping_patience\": \"10\",\n",
    "    \"cardinality\": str(cardinality),\n",
    "    \"embedding_dimension\": \"2\"\n",
    "}\n",
    "\n",
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_channels = {\n",
    "    \"train\": train_location,\n",
    "    \"test\": test_location\n",
    "}\n",
    "\n",
    "estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    endpoint_name=resource_prefix\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leveraging deployed DeepAR model endpoint for forecasting predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prediction for item with existing time series sales history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cold-start prediction for new item that has no time series sales history\n",
    "test_json = {\n",
    " \"instances\": [\n",
    "  { \"start\": \"2019-03-01 00:00:00\", \"target\": [], \"cat\": int(le.transform(['shoe'])[0])}\n",
    " ],\n",
    " \"configuration\": {\n",
    "  \"num_samples\": 5,\n",
    "  \"output_types\": [\"quantiles\"],\n",
    "  \"quantiles\": [\"0.5\", \"0.9\"]\n",
    " }\n",
    "}\n",
    "\n",
    "predictor.predict(json.dumps(test_json).encode('utf-8'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
